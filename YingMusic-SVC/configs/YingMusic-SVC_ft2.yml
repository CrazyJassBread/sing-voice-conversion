preprocess_params:
  sr: 44100
  spect_params:
    n_fft: 2048
    win_length: 2048
    hop_length: 512
    n_mels: 128
    fmin: 0
    fmax: "None"

model_params:
  dit_type: "DiT" # uDiT or DiT
  reg_loss_type: "l1" # l1 or l2

  timbre_shifter:
    timbre_shifter_type: "rvc"         # ToneColorConverter | rvc
    pool_size: 3
    se_db_path: "./modules/openvoice/checkpoints_v2/converter/se_db.pt"
    ckpt_path: "./modules/openvoice/checkpoints_v2/converter"
    use_timbre_shifter: true

  vocoder:
    type: "bigvgan"
    name: "nvidia/bigvgan_v2_44khz_128band_512x"

  speech_tokenizer:
    type: "whisper"
    name: 'openai/whisper-small'

  style_encoder:
    dim: 192
    campplus_path: "campplus_cn_common.bin"

  DAC:
    encoder_dim: 64
    encoder_rates: [2, 5, 5, 6]
    decoder_dim: 1536
    decoder_rates: [ 6, 5, 5, 2 ]
    sr: 24000

  length_regulator:
    channels: 768
    is_discrete: false
    in_channels: 768
    content_codebook_size: 2048
    sampling_ratios: [1, 1, 1, 1]
    vector_quantize: false
    n_codebooks: 1
    quantizer_dropout: 0.0
    f0_condition: true
    n_f0_bins: 256
    use_style_residual: true
    residual_alpha: 0.5

  DiT:
    hidden_dim: 768
    num_heads: 12
    depth: 17
    class_dropout_prob: 0.1
    block_size: 8192
    in_channels: 128
    style_condition: true
    final_layer_type: 'mlp'
    target: 'mel' # mel or codec
    content_dim: 768
    content_codebook_size: 1024
    content_type: 'discrete'
    f0_condition: true
    n_f0_bins: 256
    content_codebooks: 1
    is_causal: false
    long_skip_connection: false
    zero_prompt_speech_token: false # for prompt component, do not input corresponding speech token
    time_as_token: false
    style_as_token: false
    uvit_skip_connection: true
    add_resblock_in_transformer: false

  wavenet:
    hidden_dim: 768
    num_layers: 8
    kernel_size: 5
    dilation_rate: 1
    p_dropout: 0.2
    style_condition: true

  # SFA-LoRA Configuration (Spectral-Feature-Aware Low-Rank Adaptation)
  sfa_lora:
    enabled: true
    rank: 32                          # LoRA rank (r) - lower = fewer params
    alpha: 8.0                      # LoRA scaling factor
    target_modules:                  # Which attention projections to adapt
      - "wqkv"
      - "wo"
    dropout: 0.0                     # Dropout for LoRA layers
    gate_init_type: "frequency_prior"  # 'uniform', 'ones', or 'frequency_prior'
    gate_mode: "feature_wise"        # NEW: 'feature_wise', 'learned_projection', or 'direct'
    # Frequency gate learns different adaptation strengths:
    # - Low (<500Hz): Fundamental band, shared physics, low adaptation
    # - Mid (1-3kHz): Formant band, character voice, high adaptation
    # - High (>5kHz): Breath/air band, anime texture, high adaptation

  # RVA Configuration (Residual Velocity Adapter)
  rva:
    enabled: true
    hidden_channels: 256             # Hidden layer size
    use_unet: true                   # Use U-Net architecture (vs simple ResNet)
    guidance_schedule: "cosine"      # 'linear', 'cosine', 'quadratic', or 'learned'
    alpha_min: 0.0                   # Min guidance scale (at t=0)
    alpha_max: 0.5                   # Max guidance scale (at t=1)
    # Time-dependent scaling: α(t) is higher near t=1 for fine texture generation
    # Early steps (t→0): Base model handles melody/structure
    # Late steps (t→1): RVA takes over for anime texture details

  # Adapter Training Configuration
  adapter_training:
    mode: "both"                     # 'lora_only', 'rva_only', 'both', or 'full'
    freeze_base: true                # Freeze base model weights
    lora_lr_multiplier: 1.0          # Learning rate multiplier for LoRA
    rva_lr_multiplier: 1.0           # Learning rate multiplier for RVA

loss_params:
  use_balance_loss: true
  base_lr: 0.0001
  lambda_mel: 45
  lambda_kl: 1.0
